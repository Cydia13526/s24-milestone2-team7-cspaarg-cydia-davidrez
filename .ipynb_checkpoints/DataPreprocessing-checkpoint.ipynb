{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a390d0-409f-40d5-866c-e9254c98c469",
   "metadata": {},
   "source": [
    "# **SIADS Milestone II - Forecasting Federal Funds Rate Movements through Natural Language Processing of FOMC Minutes**\n",
    "\n",
    "**s24-milestone2-team7-cspaarg-cydia-davidrez**\n",
    "- **Casey Spaargaren(cspaarg@umich.edu)**, School of Information, University of Michigan\n",
    "- **Cydia Tsang (cydia@umich.edu)**, School of Information, University of Michigan\n",
    "- **David Rezkalla(davidrez@umich.edu)**, School of Information, University of Michigan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa9d02-3ff2-4c2c-8bd0-5efce1516d87",
   "metadata": {},
   "source": [
    "#### **Strcuture of the Code:**\n",
    "\n",
    "&emsp; **Data Import**<br>\n",
    "&emsp;&emsp;&emsp; Federal Reserve's meeting minutes from 2000 to 2024<br>\n",
    "&emsp; **Data Cleaning & Manipulation**<br>\n",
    "&emsp;&emsp;&emsp; 1. Basic Desciptive Statistic Data Manipulation<br>\n",
    "&emsp; **Data Analysis & Visualisation**<br>\n",
    "&emsp;&emsp;&emsp; 1. Basic Desciptive Statistic Analysis<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83204ec0-c294-4b8c-a8a3-8fe7a766aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remeber to run the following commannd before running the following codes. Details Please refer to README.md\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907e6a6a-9342-44b1-8c35-90cd0abb06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from time import sleep\n",
    "import re,csv,os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455513d-b925-4ca9-bb9b-a259c7866ca7",
   "metadata": {},
   "source": [
    "# **Data Import**\n",
    "\n",
    "The project requires the Federal Reserve's meeting minutes from 2000 to 2024, sourced from the official Fed website at https://www.federalreserve.gov. The Federal Open Market Committee (FOMC) holds eight regularly scheduled meetings each year, with additional meetings as needed. Policy statements and minutes are linked in the calendars on the website. The minutes of regularly scheduled meetings are released three weeks after the policy decision date, and committee membership changes occur at the first meeting of each year. The retrieved data is organized and stored as individual files, named meeting_minute_YYYYMMDD.txt, in the data/output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab7c54b-eb0e-4a15-bc53-f46782fadbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "releaseDates = [line.rstrip() for line in open(os.path.join('data/meeting_minute/meeting_dates.csv'), 'r')]\n",
    "\n",
    "def getFedMeetingMinutesUrl(date):\n",
    "    dateInt = int(date)\n",
    "    if dateInt <= 20070918 :\n",
    "        url = 'https://www.federalreserve.gov/fomc/minutes/' + date + '.htm'\n",
    "    elif dateInt == 20080625:\n",
    "        url = 'https://www.federalreserve.gov/monetarypolicy/fomc20080625.htm'\n",
    "    elif dateInt > 20070918:\n",
    "        url = 'https://www.federalreserve.gov/monetarypolicy/fomcminutes' + date + '.htm'\n",
    "    print(url)\n",
    "    return url\n",
    "\n",
    "def getStatement(date):\n",
    "    print('Pulling meeting minute of date: ' + date)\n",
    "    req = urllib.request.Request(getFedMeetingMinutesUrl(date), headers={'User-Agent' : \"Magic Browser\"}) \n",
    "    html = urllib.request.urlopen( req ).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    minutesText = soup.get_text(\" \")\n",
    "    return minutesText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bed392b-ed78-45d1-a782-1f0762263454",
   "metadata": {},
   "outputs": [],
   "source": [
    "for releaseDate in releaseDates:\n",
    "    file_path=\"./data/output/meeting_minute_\" + releaseDate +\".txt\"\n",
    "    if os.path.isfile(file_path) == False:\n",
    "        data = getStatement(releaseDate)\n",
    "        sleep(2)\n",
    "    \n",
    "        f = open(file_path, 'w')\n",
    "        f.write(data)\n",
    "        f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185056ba-70fb-4331-9101-d6e9f6e25a73",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b680c908-ea81-4cfb-899a-22299397aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cydiatsang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meeting_minute</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999-05-18</th>\n",
       "      <td>FRB : FOMC Minutes - May 18 , 1999 Minutes Fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>FRB : FOMC Minutes - February 1-2 , 2000 Minut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-03-21</th>\n",
       "      <td>FRB : FOMC Minutes - March 21 , 2000 Minutes F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-16</th>\n",
       "      <td>FRB : FOMC minutes - May 16 , 2000 Minutes Fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-28</th>\n",
       "      <td>FRB : FOMC Minutes - June 27-28 , 2000 Minutes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               meeting_minute\n",
       "file_date                                                    \n",
       "1999-05-18  FRB : FOMC Minutes - May 18 , 1999 Minutes Fed...\n",
       "2000-02-02  FRB : FOMC Minutes - February 1-2 , 2000 Minut...\n",
       "2000-03-21  FRB : FOMC Minutes - March 21 , 2000 Minutes F...\n",
       "2000-05-16  FRB : FOMC minutes - May 16 , 2000 Minutes Fed...\n",
       "2000-06-28  FRB : FOMC Minutes - June 27-28 , 2000 Minutes..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "# Load FOMC meeting minutes\n",
    "meeting_minutes_dir = \"data/output\"\n",
    "meeting_minutes_files = os.listdir(meeting_minutes_dir)\n",
    "\n",
    "# Function to preprocess text (tokenize and remove stop words)\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "meeting_minutes = []\n",
    "for file in meeting_minutes_files:\n",
    "    with open(os.path.join(meeting_minutes_dir, file), 'r') as f:\n",
    "        text = f.read()\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        meeting_minute = {\n",
    "            'meeting_minute': preprocessed_text,\n",
    "            'file_date': file.replace('meeting_minute_', '').replace('.txt', '')\n",
    "        }\n",
    "        meeting_minutes.append(meeting_minute)\n",
    "        \n",
    "meeting_minutes_df = pd.DataFrame(meeting_minutes)\n",
    "meeting_minutes_df.file_date = pd.to_datetime(meeting_minutes_df.file_date, format='%Y%m%d')\n",
    "meeting_minutes_df = meeting_minutes_df.set_index('file_date')\n",
    "meeting_minutes_df = meeting_minutes_df.sort_index()\n",
    "# Split the dataset into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "meeting_minutes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd89e042-c700-49c2-943d-b1da13059052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1954-07-07</th>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-07-14</th>\n",
       "      <td>1.22</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-07-21</th>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-07-28</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-08-04</th>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            value  diff\n",
       "date                   \n",
       "1954-07-07   1.00   NaN\n",
       "1954-07-14   1.22  0.22\n",
       "1954-07-21   0.57 -0.65\n",
       "1954-07-28   0.63  0.06\n",
       "1954-08-04   0.27 -0.36"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Federal Funds Rate data\n",
    "fed_fund_rate_data = pd.read_csv(\"data/fed_fund_rate.csv\")\n",
    "fed_fund_rate_data['date'] = pd.to_datetime(fed_fund_rate_data['date'])\n",
    "fed_fund_rate_data['diff'] =fed_fund_rate_data['value'].diff(1)\n",
    "fed_fund_rate_data = fed_fund_rate_data.set_index('date')\n",
    "fed_fund_rate_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efe7f863-cbba-4c2d-b2fd-45cf3dc55f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate if a rate changed within x days (try 7?) after a document is published, \n",
    "# by merging the rate and content df's\n",
    "# then display what it gets us\n",
    "\n",
    "tol = pd.Timedelta('7 day')\n",
    "merge_df = pd.merge_asof(left=meeting_minutes_df,right=fed_fund_rate_data,right_index=True,left_index=True,direction='nearest',tolerance=tol)\n",
    "merge_df.head()\n",
    "\n",
    "# Prepare the dataset\n",
    "X = meeting_minutes\n",
    "\n",
    "y = fed_fund_rate_data['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e0296-9972-4258-8502-28e7098abcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: This file converts the raw FOMC Statements into cleaner\n",
    "#              versions of themselves.\n",
    "#\n",
    "#              This file leverages an open source version for the cleaning.\n",
    "#              The content has been modified from it's originally published version and\n",
    "#              adapted for python3. The author of that original version is\n",
    "#              Miguel Acosta  www.acostamiguel.com\n",
    "#\n",
    "# Input:       The raw FOMC statements, downloaded from federalreserve.gov\n",
    "#              by the python script pullStatements.py. These are\n",
    "#              located in the directory statements/statements.raw\n",
    "#\n",
    "# Output:      Two sets of cleaned FOMC statements:\n",
    "#                (i) A set, located in the directory statements/statements.clean\n",
    "#                    of FOMC statements that have had header, footer, and voting\n",
    "#                    information removed. These files are currently being used\n",
    "#                    in this project.\n",
    "#                (ii) A set, located in statements/statements.clean.np\n",
    "#                    of FOMC statements that have had header, footer, and voting\n",
    "#                    information removed. They have also been stemmed, words\n",
    "#                    have been concatenated, and numbers/stopwords\n",
    "#                    have been removed. These files are not currently being used\n",
    "#                    in this project.\n",
    "#\n",
    "#--------------------------------- IMPORTS -----------------------------------#\n",
    "import os, csv, re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from textmining_withnumbers import TermDocumentMatrix as TDM\n",
    "\n",
    "#-------------------------DEFINE GLOBAL VARIABLES-----------------------------#\n",
    "# Directory where the stop words and n-grams to concatenate are\n",
    "datadir      = 'data'\n",
    "# Where the raw statements are\n",
    "statementdir = os.path.join('statements','statements.raw')\n",
    "# Where the clean statements will go (with and without preprocessing)\n",
    "cleanDir     = os.path.join('statements','statements.clean')\n",
    "cleanDirNP   = os.path.join('statements','statements.clean.np')\n",
    "# Where the cleaned documents should go\n",
    "outputDir    = 'output'\n",
    "\n",
    "#-----------------------------------------------------------------------------#\n",
    "# getReplacementList: Returns two lists, a list of N n-grams (phrase with n\n",
    "#   words) and a list with N \"words\" to replace the n-grams. The function reads\n",
    "#   a file, list_name, where every odd entry is an n-gram, and every even entry\n",
    "#   is a replacement.\n",
    "#-----------------------------------------------------------------------------#\n",
    "def getReplacementList (list_name):\n",
    "    allWords = [line.rstrip('\\n') for line in  open(list_name, 'r') ]\n",
    "    oldWords = [allWords[i] for i in range(len(allWords)) if i % 2 == 0]\n",
    "    newWords = [allWords[i] for i in range(len(allWords)) if i % 2 == 1]\n",
    "    return [oldWords, newWords]\n",
    "\n",
    "#-----------------------------------------------------------------------------#\n",
    "# cleanStatement: This function is the meat of this code--it performs all of the\n",
    "#   cleaning/preprocessing described in the header of this document. It's\n",
    "#   inputs are:\n",
    "#     (1) statement   : a string with the filename of a single FOMC statement\n",
    "#     (2) locationold : Directory where raw statements are located (string)\n",
    "#     (3) replacements: Output from getReplacementList\n",
    "#     (4) locationnew : Directory where clean statements go (string)\n",
    "#     (5) stoplist    : A list of words to remove (list of strings)\n",
    "#     (6) charsToKeep : A regular expression of the character types to keep\n",
    "#-----------------------------------------------------------------------------#\n",
    "def cleanStatement (statement, locationold, replacements, locationnew, \\\n",
    "                    stoplist, charsToKeep):\n",
    "    # Read in the statement and convert it to lower case\n",
    "    original  = open(os.path.join(locationold,statement),'r').read().lower()\n",
    "\n",
    "    clean = original\n",
    "    # Remove punctuation and newlines first, to keep space between words\n",
    "    for todelete in ['.', '\\r\\n', '\\n', ',', '-', ';', ':']:\n",
    "        clean = clean.replace(todelete, ' ')\n",
    "\n",
    "    # Keep only the characters that you want to keep\n",
    "    clean = re.sub(charsToKeep, '', clean)\n",
    "    clean = clean.replace('  ', ' ')\n",
    "    clean = clean.replace(' u s ', ' unitedstates ')\n",
    "\n",
    "    # Remove anything before (and including) 'for immediate release'\n",
    "    deleteBefore= re.search(\"[Ff]or\\s[Ii]mmediate\\s[Rr]elease\", \\\n",
    "                            clean).start() + len ('for immediate release')\n",
    "    clean = clean[deleteBefore:]\n",
    "\n",
    "    # Looking for the end of the text\n",
    "    intaking   = re.search(\"in\\staking\\sthe\\sdiscount\\srate\\saction\",\\\n",
    "                           clean)\n",
    "    votingfor  = re.search(\"voting\\sfor\\sthe\\sfomc\", clean)\n",
    "    if intaking == None and not votingfor == None:\n",
    "        deleteAfter = votingfor.start()\n",
    "    elif votingfor == None and not intaking == None:\n",
    "        deleteAfter = intaking.start()\n",
    "    elif votingfor == None and intaking == None:\n",
    "        deleteAfter = len(clean)\n",
    "    else:\n",
    "        deleteAfter = min(votingfor.start(), intaking.start())\n",
    "    clean = clean[:deleteAfter]\n",
    "\n",
    "    # Replace replacement words (concatenations)\n",
    "    for word in range(len(replacements[0])):\n",
    "        clean = clean.replace(replacements[0][word], replacements[1][word])\n",
    "\n",
    "    # Remove stop words\n",
    "    for word in stoplist:\n",
    "        clean = clean.replace(' '+word.lower() + ' ', ' ')\n",
    "\n",
    "    # Write cleaned file\n",
    "    new = open(os.path.join(locationnew,statement), 'w')\n",
    "    new.write(clean)\n",
    "    new.close\n",
    "\n",
    "#-----------------------------------------------------------------------------#\n",
    "# The Main function generates the stop list, and word replacement lists, then\n",
    "#   loops through every file in the statements/statements.raw directory and\n",
    "#   performs two types of cleaning: one that is less extensive (saved in\n",
    "#   statements/statements.clean) and one that includes more preprocessing steps\n",
    "#   (saved in statements/statements.clean.np). Finally, it creates the\n",
    "#   term-document matrix for each type of cleaning. 'NP' denotes 'no preprocessing.\n",
    "#\n",
    "#   Only the files in the statements/statements.clean folder are currently\n",
    "#   being used in this project, these are the files with less preprocessing\n",
    "#   at this step. More processing happens in the Jupyter Notebook code which\n",
    "#   is the file called Data.ipynb.\n",
    "#-----------------------------------------------------------------------------#\n",
    "\n",
    "def main():\n",
    "    stoplist       = [line.rstrip('\\n') for line in \\\n",
    "                      open(os.path.join(datadir,\"stoplist_mcdonald_comb.txt\")\n",
    "                           , 'r') ]\n",
    "    stoplistNP     = [line.rstrip('\\n') for line in \\\n",
    "                      open(os.path.join(datadir,\"emptystop.txt\"), 'r') ]\n",
    "\n",
    "    replacements   = getReplacementList(os.path.join(datadir,\"wordlist.txt\"))\n",
    "    replacementsNP = getReplacementList(os.path.join(datadir,\"wordlist.np.txt\"))\n",
    "\n",
    "    statementList  = [ f for f in listdir(statementdir) \\\n",
    "                       if isfile(join(statementdir,f)) ]\n",
    "\n",
    "    for statement in statementList:\n",
    "        # First, the case with heavier preprocessing (keep only letters)\n",
    "        cleanStatement(statement, statementdir, replacements, \\\n",
    "                       cleanDir, stoplist, '[^A-Za-z ]+',1)\n",
    "        # Second, the no-preprocessing case (keep letters and numbers)\n",
    "        cleanStatement(statement, statementdir, replacementsNP, \\\n",
    "                       cleanDirNP, stoplistNP, '[^A-Za-z0-9 ]+',0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6610bf-6ab1-4eff-b286-887ec2acb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to read and tokenize text from meeting minutes file\n",
    "def process_minutes(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        tokens = word_tokenize(text.lower())  # Tokenize text and convert to lowercase\n",
    "        # Remove stopwords and punctuation\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]\n",
    "        return filtered_tokens\n",
    "\n",
    "# Directory containing meeting minutes files\n",
    "minutes_dir = 'data/output/'\n",
    "\n",
    "# List to store tokenized words from all meeting minutes\n",
    "meeting_minutes = []\n",
    "\n",
    "# Process each meeting minute file\n",
    "for file_path in glob.glob(os.path.join(minutes_dir, 'meeting_minute_*.txt')):\n",
    "    tokens = process_minutes(file_path)\n",
    "    meeting_minutes.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ee1b7-ffa6-48ce-8e7a-d0c942c581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_funds_data = pd.read_csv('data/fed_fund_rate.csv')\n",
    "fed_funds_data['date'] = pd.to_datetime(fed_funds_data['date'])\n",
    "print(fed_funds_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99db5e-0842-4375-bb10-54e48d7140ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.DataFrame(columns=['date', 'tokens', 'value'])\n",
    "    for i, tokens in enumerate(meeting_minutes):\n",
    "        date_str = os.path.basename(meeting_files[i]).split('_')[-1].split('.')[0]\n",
    "        date = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "        value = fed_fund_rate_df.loc[fed_fund_rate_df['date'] == date, 'value'].values[0]\n",
    "        merged_data = merged_data.append({'date': date, 'tokens': tokens, 'value': value}, ignore_index=True)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(merged_data['tokens'], merged_data['value'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac2830-538b-4c48-b16f-d0c35a9b58f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
